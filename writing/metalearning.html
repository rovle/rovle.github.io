<!doctype html>
<html lang="en-us">

    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta http-equiv="X-UA-Compatible" content="ie=edge">
        <meta name="theme-color" content="#FE5185">
        <meta name="msapplication-TileColor" content="#FE5185">

        <meta itemprop="name" content="GPT-3" In-context model fitting experiments · Lovre Pesut>
        <meta itemprop="description" content="Can" GPT-3 fit models in-context?>

        
        <meta property="og:title" content="GPT-3" In-context model fitting experiments · Lovre Pesut />
        <meta property="og:description" content="Can" GPT-3 fit models in-context? />
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary" />
        <meta name="twitter:title" content="GPT-3" In-context model fitting experiments · Lovre Pesut />
        <meta name="twitter:description" content="Can" GPT-3 fit models in-context? />

        <link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
        <link rel="manifest" href="../site.webmanifest">
        <link rel="mask-icon" href="../safari-pinned-tab.svg" color>
        <link rel="shortcut icon" href="../favicon.ico">

        <title>GPT-3 In-context model fitting experiments · Lovre Pesut</title>

        <link rel="stylesheet" href="../css/style.css">
        <link href="https://lovrepesut.com/fancybox.css" rel="stylesheet">
        <link href="../css/footnotes.css" rel="stylesheet">
        <link href="../css/horizontal_ruler.css" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans|Unica+One&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://use.typekit.net/ssc3wax.css">
        <link href="https://fonts.googleapis.com/css2?family=Old+Standard+TT&display=swap" rel="stylesheet">
    </head>

    <body id="page">

        <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>

<script type="text/javascript" src="../js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>

  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="../" class="logo"></a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
    </div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="../">
        <li class="mobile-menu-item">Home</li>
      </a>
      <a href="../writing/alignment">
          <li class="mobile-menu-item">Alignment</li>
        </a>
        <a href="../writing/">
            <li class="mobile-menu-item">Writing</li>
        </a>
      <a href="../contact">
        <li class="mobile-menu-item">Contact</li>
      </a>
  </ul>
</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="../" class="logo"></a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="../">Home</a>
      </li>
      <li class="menu-item">
        <a class="menu-item-link" href="../writing/alignment">Alignment</a>
      </li>
        <li class="menu-item">
        <a class="menu-item-link" href="../writing/">Writing</a>
      </li>
      <li class="menu-item">
        <a class="menu-item-link" href="../contact">Contact</a>
      </li>
  </ul>
</nav>

    </header>

    	<main class="site-main section-inner thin animated fadeIn faster">
		<div class="content">
            <h1 id="gpt-3-in-context-model-fitting-experiments">GPT-3 In-context model fitting experiments</h1>
<h2 id="introduction">Introduction</h2>
<p>Much has been written and much has been observed about the abilities of GPT-3 on <a href="https://www.lesswrong.com/posts/6Hee7w2paEzHsD6mn/collection-of-gpt-3-results">many tasks</a>. Most of these showcased capabilities, though not all, pertain to <strong>writing convicing text</strong> – such as <a href="https://www.lesswrong.com/posts/bDMoMvw2PYgijqZCC/i-wanted-to-interview-eliezer-yudkowsky-but-he-s-busy-so-i">simulating Eliezer Yudkowsky</a> – but, not to undermine GPT-3’s impressiveness at performing these tasks, we might call this the <em>predictable</em> part of its ouevre. It only makes sense that a better language modelling is, well, going to be better at writing text.</p>
<p>Deeper and comparatively much less explored is the <em>unpredictable</em> ability of GPT-3 to learn new tasks by just <strong>seeing a few examples</strong>, without any training/backpropagation – the so called in-context learning (sometimes called metalearning).</p>
<p>The <a href="https://arxiv.org/abs/2005.14165">original paper announcing GPT-3</a> contained a handful of examples (perhaps mostly notably examples of GPT-3 learning to perform arithmetic, e.g. accurate addition of up to 5-digit numbers), Gwern has also insightfully <a href="https://www.gwern.net/Scaling-hypothesis#meta-learning">written about it</a>, Frieda Rong has performed <a href="http://ai.stanford.edu/blog/in-context-learning/">some interesting experiments</a>, and there have been various <a href="https://generative.ink/posts/list-sorting-does-not-play-well-with-few-shot/">other</a> <a href="https://arxiv.org/abs/2101.06804">experiments</a> one could chance upon. My curiousity being piqued but not sated by these experiments, and also having had the feeling that, as captivating the arithmetic examples were, they weren’t the most <em>natural</em> question one could ask about a stochastic model’s quantitative capabilities – I decided to investigate whether GPT-3 could <em>fit models in-context</em>.</p>
<h2 id="the-setup">The Setup</h2>
<p>What does it mean to fit a model in-context? Well, recall that GPT-3 has a context window (roughly speaking: text it considers before generating additional text) of length 2048 tokens (roughly speaking: words, punctuation, etc.) so the idea is to put feature vectors <em>inside</em> that context window. Of course, this means you cannot fit any larger or higher-dimensional dataset in there.<label for="sn-1" class="margin-toggle sidenote-number"> </label> <input type="checkbox" id="sn-1" class="margin-toggle" /> <span class="sidenote">Except perhaps through the use of <a href="https://www.lesswrong.com/posts/oBpebs5j5ngs3EXr5/a-summary-of-anthropic-s-first-paper-3#Context_Distillation">context distillation</a> or something like that, though at that point you’re not really doing purely in-context learning anymore.</span></p>
<p>In practice this means prompting GPT-3 on input like:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>Input: <span class="dv">94</span>, <span class="dv">47</span>, <span class="dv">84</span>, <span class="dv">31</span>, output <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>Input: <span class="dv">89</span>, <span class="dv">51</span>, <span class="dv">73</span>, <span class="dv">31</span>, output <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>[...]</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Input: <span class="dv">96</span>, <span class="dv">51</span>, <span class="dv">80</span>, <span class="dv">38</span>, output <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>Input: <span class="dv">90</span>, <span class="dv">37</span>, <span class="dv">76</span>, <span class="dv">27</span>, output <span class="op">=</span></span></code></pre></div>
<p>And then using its output, i.e. the text it generates, as the prediction.<label for="sn-2" class="margin-toggle sidenote-number"> </label> <input type="checkbox" id="sn-2" class="margin-toggle" /> <span class="sidenote">A couple more technical details: In all the experiments I performed, all the numbers were integers. The GPT-3 outputs were always sampled with temperature 0, i.e. they were deterministic – only the most probable output was considered. Hopefully some future work looks at the full distribution over outputs, but I restricted myself for simplicity.</span></p>

		</div>
	</main>

	<footer id="site-footer" class="section-inner thin animated fadeIn faster">
		<p></p>  

  <script src="../jquery.min.js"></script>
  <script src="../slideout.min.js"></script>
  <script src="../fancybox.js"></script>



<script type="text/javascript" src="../auxiliary.js"></script>



</body>

</html>
