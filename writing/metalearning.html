<!doctype html>
<html lang="en-us">

    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta http-equiv="X-UA-Compatible" content="ie=edge">
        <meta name="theme-color" content="#FE5185">
        <meta name="msapplication-TileColor" content="#FE5185">

        <meta itemprop="name" content="GPT-3" In-context model fitting experiments · Lovre Pesut>
        <meta itemprop="description" content="Can" GPT-3 fit models in-context?>

        
        <meta property="og:title" content="GPT-3" In-context model fitting experiments · Lovre Pesut />
        <meta property="og:description" content="Can" GPT-3 fit models in-context? />
        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary" />
        <meta name="twitter:title" content="GPT-3" In-context model fitting experiments · Lovre Pesut />
        <meta name="twitter:description" content="Can" GPT-3 fit models in-context? />

        <link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
        <link rel="manifest" href="../site.webmanifest">
        <link rel="mask-icon" href="../safari-pinned-tab.svg" color>
        <link rel="shortcut icon" href="../favicon.ico">

        <title>GPT-3 In-context model fitting experiments · Lovre Pesut</title>

        <link rel="stylesheet" href="../css/style.css">
        <link href="https://lovrepesut.com/fancybox.css" rel="stylesheet">
        <link href="../css/footnotes.css" rel="stylesheet">
        <link href="../css/horizontal_ruler.css" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Open+Sans|Unica+One&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=EB+Garamond&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://use.typekit.net/ssc3wax.css">
        <link href="https://fonts.googleapis.com/css2?family=Old+Standard+TT&display=swap" rel="stylesheet">

        <script type="text/javascript" id="MathJax-script" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>

    <body id="page">

        <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>

<script type="text/javascript" src="../js/main.min.c12618f9a600c40bd024996677e951e64d3487006775aeb22e200c990006c5c7.js"></script>

  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="../" class="logo"></a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
    </div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="../">
        <li class="mobile-menu-item">Home</li>
      </a>
      <a href="../writing/alignment">
          <li class="mobile-menu-item">Alignment</li>
        </a>
        <a href="../writing/">
            <li class="mobile-menu-item">Writing</li>
        </a>
      <a href="../contact">
        <li class="mobile-menu-item">Contact</li>
      </a>
  </ul>
</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="../" class="logo"></a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="../">Home</a>
      </li>
      <li class="menu-item">
        <a class="menu-item-link" href="../writing/alignment">Alignment</a>
      </li>
        <li class="menu-item">
        <a class="menu-item-link" href="../writing/">Writing</a>
      </li>
      <li class="menu-item">
        <a class="menu-item-link" href="../contact">Contact</a>
      </li>
  </ul>
</nav>

    </header>

    	<main class="site-main section-inner thin animated fadeIn faster">
		<div class="content">
            <h1 id="who-models-the-models-that-model-models-an-exploration-of-gpt-3s-in-context-model-fitting-ability">Who models the models that model models? An exploration of GPT-3’s in-context model fitting ability</h1>
<h2 id="introduction">Introduction</h2>
<p>Much has been written and much has been observed about the abilities of GPT-3 on <a href="https://www.lesswrong.com/posts/6Hee7w2paEzHsD6mn/collection-of-gpt-3-results">many tasks</a>. Most of these capabilities, though not all, pertain to <strong>writing convicing text</strong>, but, not to undermine GPT-3’s impressiveness at performing these tasks, we might call this the <em>predictable</em> part of its oeuvre. It only makes sense that a better language modelling is, well, going to be better at writing text.</p>
<p>Deeper and comparatively much less explored is the <em>unpredictable</em> ability of GPT-3 to learn new tasks by just <strong>seeing a few examples</strong>, without any training/backpropagation – the so called in-context learning (sometimes called metalearning).</p>
<p>The <a href="https://arxiv.org/abs/2005.14165">original paper announcing GPT-3</a> contained a handful of examples (perhaps mostly notably examples of GPT-3 learning to perform arithmetic, e.g. accurate addition of up to 5-digit numbers), Gwern has also insightfully <a href="https://www.gwern.net/Scaling-hypothesis#meta-learning">written about it</a>, Frieda Rong has performed <a href="http://ai.stanford.edu/blog/in-context-learning/">some interesting experiments</a>, and there have been various <a href="https://generative.ink/posts/list-sorting-does-not-play-well-with-few-shot/">other</a> <a href="https://arxiv.org/abs/2101.06804">experiments</a> one could chance upon. My curiosity being piqued but not sated by these experiments, and also having had the feeling that, as captivating the arithmetic examples were, they weren’t the most <em>natural</em> question one could ask about a stochastic model’s quantitative capabilities – I decided to investigate whether GPT-3 could <em>fit numerical models in-context</em>.</p>
<h2 id="the-setup">The Setup</h2>
<p>What does it mean to fit a model in-context? Well, recall that GPT-3 has a context window (roughly speaking: text it considers before generating additional text) of length 2048 tokens (roughly speaking: (parts of the) words, punctuation, etc.) so the idea is to put feature vectors <em>inside</em> that context window. Of course, this means you cannot fit any larger or higher-dimensional dataset in there.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>In practice this means prompting GPT-3 on input like:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="an">Input:</span><span class="co"> 94, 47, 84, 31, output = 2</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">Input:</span><span class="co"> 89, 51, 73, 31, output = 1</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">...</span><span class="co">]</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>Input: 96, 51, 80, 38, output = 2</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>Input: 90, 37, 76, 27, output =</span></code></pre></div>
<p>And then taking its output, i.e. the text it generates, as the prediction.</p>
<p>A couple more technical details: In all the experiments I performed, all the numbers were integers. The GPT-3’s outputs were always sampled with temperature 0, i.e. they were deterministic – only the most probable output was considered. I restricted myself that way for simplicity, but hopefully some future work looks at the full distribution over outputs.</p>
<p>(In the rest of this post I share GPT-3’s results without too much warning, so if you’d like to make your own predictions about how GPT-3 does at simple classification and regression tasks in-context, now would be the time to <strong>pause reading and make your general predictions.</strong>)</p>
<h2 id="iris-dataset">Iris Dataset</h2>
<p>Just as there is the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a> for visual recognition, so too there is a small low-dimensional classification dataset which every would-be classifier has to solve or else sink – the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris dataset</a>, composed of 150 observations of septal/petal height/width of three species of iris (50 observations each). Note that classification of the Iris dataset is in some sense trivial, with simple algorithms achieving near-perfect accuracy, but a model still needs to do some meaningful learning, given that it needs to learn to differentiate between three classes based on just a few dozen four-dimensional vectors.</p>
<p>In order to prevent leakage, as the Iris dataset is all over the internet, as well as to get the more-easily-palatable integers out, I transformed every feature with the transformation</p>
<p>\[ x_{\text{new}} = \operatorname{round}(14x_{\text{old}} + 6) . \]</p>
<p>I also split the dataset into 50% train and 50% test folds – so that the model “trained” on, or rather <em>looked</em> at 75 examples.</p>
<p>I hadn’t quite been expecting what had followed – I had expected GPT-3 to catch on to some patterns, to have a serviceable but not-quite-impressive accuracy – instead, the accuracies, averaged over 5 random dataset shufflings, for Ada (350M params), Babbage (1.3B), Curie (6.7B), and Davinci (175B), compared to kNN (k=5) and logistic regression were:</p>
<p><span class="math display">$$
\begin{array}{|c|c|}
\hline
\text{Model} &amp; \text{Accuracy} \\
\hline
\text{kNN} &amp; 95.73\% \\
\hline
\text{Logistic regr.} &amp; 96.26\% \\
\hline
\text{Ada} &amp; 89.86\% \\
\hline
\text{Babbage} &amp; 93.06\% \\
\hline
\text{Curie} &amp; 95.20\% \\
\hline
\text{Davinci} &amp; 95.73\% \\
\hline
\end{array}
$$</span> So Curie and Davinci did about as well as kNN<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> and logistic regression. GPT-3, just by <em>looking</em> at feature vectors, solves Iris.</p>
<p>I also conducted a few other experiments with the Iris dataset. One was not labelling the “input” or “output”, but just sending bare numbers, like this:</p>
<pre><code>94, 47, 84, 31, 2
89, 51, 73, 31, 1
[...]
91, 48, 75, 31, 2
96, 51, 80, 38,	</code></pre>
<p>This seemed to degrade the results very slightly. Scaling up numbers so that all features were in the hundreds also seemed to potentially degrade performance by a few percentage points, but I didn’t investigate that exhaustively either.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<h2 id="d-binary-classification-generally">2D binary classification, generally</h2>
<p>These results, while interesting, only show GPT-3’s model-fitting ability on one dataset. It might, indeed, be that the Iris dataset is, for some reason, unusually easy to classify for GPT-3. That’s why I conducted some more experiments, in a lower dimension, where I could try out a lot different things.</p>
<p>To the end of trying out a lot of things, I constructed 9 “typical binary classification scenarios” – I tried to think up a set of class distributions which would capture a decent number of realistic-looking two-class binary classification cases, as well as some slightly adversarial ones – e.g. scenario 7 is the analogue of the “XOR” problem, which perceptrons famously cannot learn because they define a linear boundary. Below you can see a random sample of each of these scenarios.</p>
<p><img src="../assets/distribucije.png" /></p>
<p>For each scenario I sampled a dataset three times; each of those datasets had 50 “train” examples and 30 test examples. Here are the results, comparing between kNN (k=5<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>), logistic regression, a custom text-based classifier which I wrote thinking about the easiest text-based-algorithm GPT could learn<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, and GPT-3.</p>
<p><span class="math display">$$
\begin{array}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\text{Model} &amp; \text{Scen. 1} &amp; \text{Scen. 2} &amp; \text{Scen. 3} &amp; \text{Scen. 4} &amp; \text{Scen. 5} &amp; \text{Scen. 6} &amp; \text{Scen. 7} &amp; \text{Scen. 8} &amp; \text{Scen. 9}\\
\hline
\text{kNN} &amp; 75.56\% &amp; \textbf{78.89%} &amp; 71.11\% &amp; \textbf{93.33%} &amp; \textbf{98.89%} &amp; 75.56\% &amp; \textbf{90.0%} &amp; \textbf{83.33%} &amp; \textbf{68.89%} \\
\hline
\text{Logistic regr.}  &amp; 75.56\% &amp; \textbf{78.89%} &amp; 46.67\% &amp; \textbf{93.33%} &amp; 38.89\% &amp; \textbf{81.11%} &amp; 47.78\% &amp; 51.11\% &amp; 47.78\% \\
\hline
\hline
\text{Custom text} &amp; 70.00 \% &amp; 72.22\% &amp; 66.67\% &amp; 75.56\% &amp; 81.11\% &amp; 53.33\% &amp; 42.22\% &amp; 78.89\% &amp; 63.33\% \\
\hline
\hline
\text{Ada} &amp; \textbf{80.0%} &amp; 67.78\% &amp; \textbf{77.78%} &amp; 85.56\% &amp; 91.11\% &amp; 51.11\% &amp; 84.44\% &amp; 68.89\% &amp; 56.67\% \\
\hline
\text{Babbage} &amp; 63.33\% &amp; 62.22\% &amp; 72.22\% &amp; 91.11\% &amp; 87.78\% &amp; 55.56\% &amp; 75.56\% &amp; 74.44\% &amp; 66.67\% \\
\hline
\text{Curie} &amp; 76.67\% &amp; 71.11\% &amp; 75.56\% &amp; 86.67\% &amp; 93.33\% &amp; 73.33\% &amp; 76.67\% &amp; 64.44\% &amp; 63.33\% \\
\hline
\text{Davinci} &amp; 67.78\% &amp; 76.67\% &amp; \textbf{77.78%} &amp; 82.22\% &amp; 95.56\% &amp; 77.78\% &amp; 70.0\% &amp; 72.22\% &amp; 63.33\% \\ 
\hline
\end{array}
$$</span></p>
<p>GPT-3, as can be seen, does significantly better than chance on each of these scenarios. I would like to lightly discourage reading too much into what GPT-3 is doing just from the above numbers. Despite there being tables and graphs, this is at heart very much just an exploratory work – aiming to investigate_whether_ there is something there, not <em>what</em> exactly it is – hence the methodology here being quite lacking insofar as “drawing deeper conclusions” goal is concerned.</p>
<p>The table below shows the averages for each of the above models, noting that this makes only marginal sense, given that some of the “scenarios” are inherently harder than others. If I were doing this anew, I’d probably standardize all the “scenarios” so that distributions are such that the expected accuracy of say kNN is 80%. But it still seems better to display this, rather than not:</p>
<p><span class="math display">$$
\begin{array}{|c|c|}
\hline
\text{Model} &amp; \text{Average acc.} \\
\hline
\text{kNN} &amp; 81.78\% \\
\hline
\text{Logistic regr.} &amp; 62.34\% \\
\hline
\text{Custom text} &amp; 67.03\% \\
\hline
\text{Ada} &amp; 73.70\% \\
\hline
\text{Babbage} &amp; 72.10\% \\
\hline
\text{Curie} &amp; 75.68\% \\
\hline
\text{Davinci} &amp; 75.93\% \\
\hline
\end{array}
$$</span></p>
<p>On the graph below each dot denotes the difference in accuracy, for each model, and for each scenario and each random sample of it, compared to kNN – noting that the same disclaimer about this making only marginal sense still applies.</p>
<p><img src="../assets/razlike.png" /></p>
<h3 id="is-there-a-number-sense">Is there a number sense?</h3>
<p>One of the immediate questions one might have: is GPT-3 just operating on the basis of pure symbols, or is there some “number sense” which it is using while fitting these models? To this end, I took each of the above scenarios, and substituted all the digits in the input vectors by the (randomly-generated) mapping <span class="math inline">0 ↦ ’d’, 1 ↦ ’a’, …, 9 ↦ ’x’</span>. Hence the input looked like this:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>Input = mw, mc, output = 1</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>Input = bd, wb, output = 0</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">...</span><span class="co">]</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>Input = wm, cw, output = 1</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>Input = ch, jj, output = </span></code></pre></div>
<p>A friend pointed out that encoding issues might negatively affect results if there are no spaces between the letters, so I tried this version out too:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>Input = m w , m c , output = 1</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>Input = b d , w b , output = 0</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">...</span><span class="co">]</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>Input = w m , c w , output = 1</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>Input = c h , j j , output = </span></code></pre></div>
<p>Below, you can see the results; all the models were run with Davinci.</p>
<p><span class="math display">$$
\begin{array}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\text{Model} &amp; \text{Scen. 1} &amp; \text{Scen. 2} &amp; \text{Scen. 3} &amp; \text{Scen. 4} &amp; \text{Scen. 5} &amp; \text{Scen. 6} &amp; \text{Scen. 7} &amp; \text{Scen. 8} &amp; \text{Scen. 9}\\
\hline
\text{Letters} &amp; 48.89\% &amp; 58.89\% &amp; 72.22\% &amp; 52.22\% &amp; 84.44\% &amp; 47.78\% &amp; 47.78\% &amp; 63.33\% &amp; 54.44\% \\
\hline
\text{Letters (spaced)}&amp; 56.67\% &amp; 64.44\% &amp; 72.22\% &amp; 62.22\% &amp; 83.33\% &amp; 47.78\% &amp; 48.89\% &amp; 68.89\% &amp; 53.33\% \\
\hline
\text{Numbers} &amp; 67.78\% &amp; 76.67\% &amp; 77.78\% &amp; 82.22\% &amp; 95.56\% &amp; 77.78\% &amp; 70.0\% &amp; 72.22\% &amp; 63.33\% \\ 
\hline
\end{array}
$$</span></p>
<p>So, the letter-models do learn something; the spaced letter being on average a bit better of the two, but both being clearly inferior to the model prompted with numerical vectors.</p>
<h2 id="learning-regression">Learning regression</h2>
<p>I also ventured to test, though not nearly as extensively nor with any kind of attempt at systematicity, regression. I exclusively tested functions <span class="math inline">ℝ → ℝ</span> with added noise; the results were sometimes shockingly good, sometimes bad, but the most important part of the bottom line was that GPT-3 <strong>often successfully extrapolates</strong>.</p>
<p>I’ll showcase just a few examples, showing both ‘success’ and ‘failure’. A reminder that the input to GPT-3 in these examples was of the form:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>Input = 7, output = -131</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>Input = 24, output = -338</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">...</span><span class="co">]</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>Input = 95, output = -2270</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>Input = 13, output = </span></code></pre></div>
<p>All of the examples were run with Davinci.</p>
<p>First, we ask GPT-3 to fit</p>
<p><span class="math display">$$\begin{align}
y = -22x + 45 + \epsilon,
\end{align}$$</span></p>
<p>where <span class="math inline"><em>ϵ</em></span> is distributed normally with mean <span class="math inline">0</span> and standard deviation <span class="math inline">100</span> (<span class="math inline"><em>ϵ</em> ∼ 𝒩(0,100<sup>2</sup>)</span>). Despite a great amount of noise, only 15 examples in the “train” set, and decently large negative output numbers, GPT-3 learns it quite well:</p>
<p><img src="../assets/reg1.png" /></p>
<p>Note the bottom-most two points, where GPT-3 extrapolates faithfully to a region where it hasn’t seen any points before. This does not seem a mere coincidence, but rather – based on my limited experiments – recurs regularly.</p>
<p>Now we fit</p>
<p><span class="math display">$$\begin{align}
y = 24x + 95 + \epsilon,
\end{align}$$</span></p>
<p>where <span class="math inline"><em>ϵ</em> ∼ 𝒩(0,250<sup>2</sup>)</span>, and we also sample <span class="math inline"><em>x</em></span> for training in the range from <span class="math inline">30</span> to <span class="math inline">70</span>, while we test on randomly sampled points from <span class="math inline">0</span> to <span class="math inline">100</span>.</p>
<p><img src="../assets/reg2.png" /></p>
<p>As we can see, GPT-3 extrapolates reasonably, though starts being worse as it gets farther away from the training domain.</p>
<p>The following example tests both nonlinearity and nonmonotonicity. So, we’re modelling <span class="math display">$$\begin{align}
y = x^2 - 88x + 3 + \epsilon,
\end{align}$$</span> with <span class="math inline"><em>ϵ</em> ∼ 𝒩(0,50<sup>2</sup>)</span>. Anyway, the model does reasonably fine in the interpolating region, but does badly when extrapolating:</p>
<p><img src="../assets/reg3.png" /></p>
<p>But then, on another example,</p>
<p><span class="math display">$$\begin{align}
y = -5x^2 + 35x + 201 + \epsilon,
\end{align}
$$</span> <span class="math inline"><em>ϵ</em> ∼ 𝒩(0,600<sup>2</sup>)</span> – GPT-3 fits the data nicely, both when interpolating and extrapolating, despite really big standard deviation, output numbers into negative tens of thousands, and nonlinearity.</p>
<p><img src="../assets/reg4.png" /></p>
<p>I could share some more examples, but they would all be pretty similar to these ones. To my best judgment, I don’t think these are cherrypicked – in fact, it could be that failure is slightly overrepresented among these few examples, compared to all the ones I’ve looked at.</p>
<p>All in all, this is a pretty limited span of regression experiments, since I wasn’t pursuing any kind of systematicity in this case as in the classification case; so I’d be antsy to see more done, especially taking <strong>the full distribution of outputs in consideration</strong>, not just taking the most probable output.</p>
<h2 id="code">Code</h2>
<p>All code, all metadata and results of each experiment can be found <a href="https://github.com/rovle/gpt3-in-context-fitting">here</a>. I didn’t work much on either code readability, cleanliness or documentation, so you might find it hard to read or reuse it – it’s not meant to be used, in short, but rather just serve as a record of what I did.</p>
<h2 id="discussion">Discussion</h2>
<p>This experiment was motivated by my intuition, formed by playing with GPT-3, that there <em>is</em> some analogue of model-fitting going on in the background, probably even when one isn’t prompting it with <em>explicit</em> meta-learning-y stuff – and it seemed like a good first step towards elucidation of that hypothesis to show that there is some quite explicit model-fitting going on, that meta-learning is more than a fancy trick. And as far as that is a coherent hypothesis, this work seems to go into its favor.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<hr />
<p>It also vaguely seemed important to have ways of testing (language) model capabilities which</p>
<ul>
<li>Are language-understanding independent.</li>
<li>Can be scaled up indefinitely (i.e. can be made arbitrarily harder).</li>
<li>Are leakage-proof (that is, there is nowhere on the internet where the results might already have been written down, in some fashion).</li>
</ul>
<p>I would say the present work kind of failed to fulfill this promise, given that nothing I tried showed even noticeable differences between the 6.7B model and the 175B model – where there “should” have been a noticeable difference – however, further experiments could imaginably amend this, perhaps find sets of numerical model-fitting tasks which exhibit smoother scaling performance growth.</p>
<p>In fact, exploration of these tasks seems a potentially fruitful avenue of exploration of what is happening inside these models, or at least as a tool for understanding their metalearning abilities. There are so many experiments in this direction which I’d like to run, though I’m currently limited both by a lack of funding and the lack of time, as there are other experiments I’d like to run which seem higher-impact than merely continuing this line of investigation – but I’m very curious to hear what people on LW make of these results!</p>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Except e.g. through the use of <a href="https://www.lesswrong.com/posts/oBpebs5j5ngs3EXr5/a-summary-of-anthropic-s-first-paper-3#Context_Distillation">context distillation</a> or something like that, though at that point you’re not really doing purely in-context learning anymore.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>I went with k=5 as that’d be my first choice, and k=3 and k=7 had very similar results.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>Each of these runs cost about ~5 USD, which is why I didn’t go completely overboard with testing and exploring and I would be wont to do.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>k=3 and k=7 had very similar results.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>The classifier works by reducing every number to its tens digit, and then for each test example, each train example votes for it if its ten digits matches it – the example is classified as the class which gave it more votes. Note that while this classifier is defined in de facto textual terms, it is quite geometric in what it actually <em>does</em>, and it is quite similar to kNN in spirit – there is an implicit computation of Euclidean distance within the algorithm.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Though any reinterpretations of this work which cast a different light would be appreciated.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>I have various vague intuitions for the importance of metalearning and its implication for the alignment, but I’ll probably share those in some future essay, leaving this post to be chiefly about the experiment.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

		</div>
	</main>

	<footer id="site-footer" class="section-inner thin animated fadeIn faster">
		<p></p>  

  <script src="../jquery.min.js"></script>
  <script src="../slideout.min.js"></script>
  <script src="../fancybox.js"></script>



<script type="text/javascript" src="../auxiliary.js"></script>



</body>

</html>
